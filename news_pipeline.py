# -*- coding: utf-8 -*-
"""
news_pipeline.py (ì œëª©ë§Œ í‘œì‹œ / ì „ì—­ URL+ì œëª© ìœ ì‚¬ë„ ì¤‘ë³µ ì œê±° / ì„ íƒí˜• ì„ ë³„ëª¨ë“œ)
- Google News(RSS) ìˆ˜ì§‘
- (í•„ìˆ˜) ë„ë©”ì¸/ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì°¨ë‹¨ â†’ (ì„ íƒ) íœ´ë¦¬ìŠ¤í‹±+AI ê´€ë ¨ì„± â†’ TopN ì„ ë°œ
- ë©”ì¼ ì¹´ë“œ: ì œëª©, ê²Œì‹œ ì‹œê°(KST), ì›ë¬¸ ë§í¬
- ìµœì¢… ë‹¨ê³„ì—ì„œ 'ì œëª© ìœ ì‚¬ë„ ì „ì—­ dedupe'ë¡œ ì¤‘ë³µ ì œëª© ì œê±°
"""

import os, sys, smtplib, pytz, yaml, feedparser, requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from urllib.parse import quote_plus

from rapidfuzz.distance import Levenshtein

# ì„ íƒ ëª¨ë“œê°€ "scored"ì¼ ë•Œë§Œ ì‚¬ìš©
from utils.scoring import compute_score, apply_unrelated_penalty
from utils.relevance import is_relevant

from utils.dedupe import dedupe_items, normalize_url

try:
    from apscheduler.schedulers.blocking import BlockingScheduler
except Exception:
    BlockingScheduler = None


# ----------------- Helpers -----------------

def load_config(path="config.yaml"):
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)

def google_news_rss(query, cfg):
    base = cfg["sources"]["google_news"]["base"]
    params = {
        "q": f'{query} when:1d',
        "hl": cfg["sources"]["google_news"]["hl"],
        "gl": cfg["sources"]["google_news"]["gl"],
        "ceid": cfg["sources"]["google_news"]["ceid"],
    }
    q = "&".join([f"{k}={quote_plus(v)}" for k, v in params.items()])
    url = f"{base}?{q}"
    headers = {"User-Agent": "Mozilla/5.0 (refinery-news-bot; +https://github.com)"}
    r = requests.get(url, headers=headers, timeout=15)
    r.raise_for_status()
    return feedparser.parse(r.text)

def extract_text(entry):
    title = entry.get("title", "")
    summary_html = entry.get("summary", "")
    try:
        summary = BeautifulSoup(summary_html, "html5lib").get_text(" ", strip=True)
    except Exception:
        summary = BeautifulSoup(summary_html, "html.parser").get_text(" ", strip=True)
    return title, summary

def is_block_domain(link, cfg):
    for d in cfg["filters"]["block_domains"]:
        if d in link:
            return True
    return False

def within_window(published, tz, start_dt, end_dt):
    if not published:
        return True
    try:
        dt = datetime(*published[:6], tzinfo=pytz.utc).astimezone(tz)
        return start_dt <= dt <= end_dt
    except Exception:
        return True

def to_local_str(published, tz):
    try:
        dt = datetime(*published[:6], tzinfo=pytz.utc).astimezone(tz)
        return dt.strftime("%Y-%m-%d %H:%M KST")
    except Exception:
        return "ì‹œê°„ ì •ë³´ ì—†ìŒ"

def published_dt_kst(published_parsed, tz):
    try:
        if published_parsed:
            return datetime(*published_parsed[:6], tzinfo=pytz.utc).astimezone(tz)
    except Exception:
        pass
    # ì—†ìœ¼ë©´ ì˜› ë‚ ì§œë¡œ ì¹˜í™˜(ì •ë ¬ ë’¤ë¡œ ë³´ëƒ„)
    return datetime(1970, 1, 1, tzinfo=tz)

# ì œëª© ìœ ì‚¬ë„ ì „ì—­ ì¤‘ë³µ ì œê±° (Levenshtein similarity)
def dedupe_by_title_similarity(items, threshold=0.88):
    result = []
    seen_titles = []
    for it in items:
        title = (it.get("title") or "").strip()
        dup = False
        for seen in seen_titles:
            sim = 1 - (Levenshtein.distance(title, seen) / max(len(title), len(seen) or 1))
            if sim >= threshold:
                dup = True
                break
        if not dup:
            result.append(it)
            seen_titles.append(title)
    return result


# ----------------- Pipeline pieces -----------------

def build_query_terms(taxonomy_item):
    return taxonomy_item["keywords"]

def block_by_keywords(title, summary, cfg):
    low = f"{title} {summary}".lower()
    for w in cfg["filters"].get("block_keywords", []):
        if w.lower() in low:
            return True
    return False

def make_html_email(grouped, cfg, start_dt, end_dt):
    # ì œëª© + ê²Œì‹œ ì‹œê° + ë§í¬ ë²„íŠ¼ë§Œ ì¶œë ¥
    head = f"""
    <html><body style="font-family:Arial,Helvetica,sans-serif;">
      <h2>ì •ìœ  ë‰´ìŠ¤ ìš”ì•½ ({start_dt.strftime('%Y-%m-%d %H:%M')} ~ {end_dt.strftime('%Y-%m-%d %H:%M')} KST)</h2>
      <p style="color:#666;">ì •ìœ ì‚¬ ì§ì› ê´€ì  ìœ ì˜ë¯¸ ê¸°ì‚¬ë§Œ(ê·œì¹™/ì„ íƒì  AI) ì„ ë³„í–ˆìœ¼ë©°, ê²Œì‹œ ì‹œê°ì„ í•¨ê»˜ í‘œê¸°í•©ë‹ˆë‹¤.</p>
    """
    cards = []
    for major, minors in grouped.items():
        cards.append(f'<h3 style="border-bottom:2px solid #eee;padding-bottom:4px;">{major}</h3>')
        for minor, items in minors.items():
            if not items:
                continue
            cards.append(f'<h4 style="margin:10px 0 6px 0;color:#0a4;">{minor}</h4>')
            for it in items:
                posted = it.get("published_local", "ì‹œê°„ ì •ë³´ ì—†ìŒ")
                cards.append(f"""
                <div style="border:1px solid #eee;border-radius:10px;padding:12px;margin:8px 0;">
                  <div style="color:#888;font-size:0.9em;margin-bottom:4px;">ğŸ“… {posted}</div>
                  <div style="font-weight:600;margin-bottom:10px;">{it['title']}</div>
                  <a style="display:inline-block;background:#1565C0;color:#fff;padding:8px 12px;border-radius:6px;text-decoration:none;"
                     href="{it['link']}" target="_blank" rel="noopener">ì›ë¬¸ ë³´ê¸°</a>
                </div>
                """)
    tail = "</body></html>"
    return head + "\n".join(cards) + tail

def send_email(html, cfg):
    print(f"[SMTP] host={cfg['email']['smtp_host']} port={cfg['email']['smtp_port']} tls={cfg['email']['use_tls']}")
    msg = MIMEMultipart("alternative")
    subject = f"{cfg['email']['subject_prefix']} {datetime.now().strftime('%Y-%m-%d')}"
    msg["Subject"] = subject
    msg["From"] = f"{cfg['email']['from_name']} <{cfg['email']['from_addr']}>"
    msg["To"] = ", ".join(cfg["email"]["to_addrs"])
    msg.attach(MIMEText(html, "html", "utf-8"))

    server = smtplib.SMTP(cfg["email"]["smtp_host"], cfg["email"]["smtp_port"], timeout=20)
    if cfg["email"]["use_tls"]:
        server.starttls()
    pwd = os.getenv(cfg["email"]["password_env"], "")
    if cfg["email"]["username"]:
        print(f"[SMTP] login as {cfg['email']['username']} (pwd={'SET' if bool(pwd) else 'EMPTY'})")
        server.login(cfg["email"]["username"], pwd)
    server.sendmail(cfg["email"]["from_addr"], cfg["email"]["to_addrs"], msg.as_string())
    server.quit()


# ----------------- Main run -----------------

def run_once():
    cfg = load_config()
    tz = pytz.timezone(cfg["app"]["timezone"])
    now = datetime.now(tz)
    end_dt = now
    start_dt = end_dt - timedelta(hours=cfg["app"]["lookback_hours"])

    # ì„ íƒ ëª¨ë“œ: "recent" (ê¸°ë³¸) | "scored"
    selection_mode = (cfg.get("app", {}).get("selection_mode") or "recent").lower()

    print(f"[INFO] Window: {start_dt} ~ {end_dt} {cfg['app']['timezone']} (mode={selection_mode})")
    taxonomy = cfg["taxonomy"]
    grouped = {}
    total_raw = 0
    total_kept = 0

    # ì „ì—­ URL ì¤‘ë³µ ë°©ì§€ìš© ì„¸íŠ¸ (ì •ê·œí™” URL ê¸°ì¤€)
    global_seen_urls = set()

    # AI ì‚¬ìš© ì˜ˆì‚° (scored ëª¨ë“œì¼ ë•Œë§Œ ì‹¤ì‚¬ìš©)
    ai_budget = int(cfg.get("openai", {}).get("relevance_max_checks", 30))
    ai_used = 0
    use_ai_flag = bool(cfg.get("openai", {}).get("enable_ai_filter", False))

    for tax in taxonomy:
        major, minor = tax["major"], tax["minor"]
        grouped.setdefault(major, {})
        grouped[major].setdefault(minor, [])
        keywords = build_query_terms(tax)

        bucket = []
        for kw in keywords:
            try:
                d = google_news_rss(kw, cfg)
                got = len(d.entries)
                print(f"[FETCH] {major}/{minor}/{kw}: entries={got}")
                items = []
                for e in d.entries:
                    link = e.get("link", "")
                    if not link or is_block_domain(link, cfg):
                        continue
                    title, summary = extract_text(e)
                    if not title:
                        continue
                    published_parsed = getattr(e, "published_parsed", None)
                    if not within_window(published_parsed, tz, start_dt, end_dt):
                        continue
                    if block_by_keywords(title, summary, cfg):
                        continue
                    items.append({
                        "title": title,
                        "summary": summary,   # ë Œë”ë§ ë¯¸ì‚¬ìš©
                        "link": link,
                        "published": getattr(e, "published", ""),
                        "published_local": to_local_str(published_parsed, tz),
                        "published_dt": published_dt_kst(published_parsed, tz),
                        "source": getattr(e, "source", {}).get("title") if hasattr(e, "source") else "",
                        "major": major,
                        "minor": minor
                    })
                total_raw += len(items)
                bucket.extend(items)
            except Exception as ex:
                print(f"[WARN] fetch failed for {kw}: {ex}")

        # 1) ì†Œë¶„ë¥˜ ë‚´ ì¤‘ë³µ ì œê±°
        before = len(bucket)
        bucket = dedupe_items(bucket)
        after_dedupe = len(bucket)

        # 2) ì„ ë³„ ë¡œì§
        if selection_mode == "recent":
            # ì ìˆ˜/AI ì „í˜€ ì‚¬ìš©í•˜ì§€ ì•Šê³  'ìµœì‹ ìˆœ' ì •ë ¬
            bucket.sort(key=lambda it: it.get("published_dt"), reverse=True)
            filtered = bucket  # ì „ë¶€ í†µê³¼(ì´ë¯¸ ë¸”ë™ë¦¬ìŠ¤íŠ¸/ë„ë©”ì¸/ìœˆë„ìš° í†µê³¼í•œ ê²ƒë“¤)
            ai_used_now = 0
        else:
            # "scored": íœ´ë¦¬ìŠ¤í‹± í”„ë¦¬-ìŠ¤ì½”ì–´ â†’ ìƒìœ„ Nê°œë§Œ AI â†’ ìµœì¢… ìŠ¤ì½”ì–´ ì •ë ¬
            prelims = []
            hitset = set(keywords)
            for it in bucket:
                txt = f"{it['title']} {it.get('summary','')}"
                pre = compute_score(txt, cfg) + apply_unrelated_penalty(hitset, txt, cfg)
                it["_pre_score"] = pre
                prelims.append(it)
            prelims.sort(key=lambda x: x["_pre_score"], reverse=True)

            can_use = max(0, ai_budget - ai_used)
            top_n = min(can_use, len(prelims))

            filtered = []
            cfg_no_ai = {**cfg, "openai": {**cfg.get("openai", {}), "enable_ai_filter": False}}
            ai_used_now = 0
            for idx, it in enumerate(prelims):
                txt = f"{it['title']}. {it.get('summary','')}"
                if idx < top_n and use_ai_flag:
                    rel = is_relevant(txt, cfg)     # ë‚´ë¶€ì—ì„œ ì‹¤íŒ¨ ì‹œ íœ´ë¦¬ìŠ¤í‹±
                    ai_used += 1
                    ai_used_now += 1
                else:
                    rel = is_relevant(txt, cfg_no_ai)
                if rel:
                    filtered.append(it)

            # ìµœì¢… ìŠ¤ì½”ì–´ ê¸°ë°˜ ì •ë ¬
            # (ì£¼ì˜: ì—¬ê¸°ì„œëŠ” compute_score/apply_unrelated_penaltyê°€ ì´ë¯¸ ë°˜ì˜ë˜ì–´ ìˆìŒ)
            # ì¢€ ë” ì‹ ë¢°í•˜ë ¤ë©´ ë‹¤ì‹œ ì •ë ¬
            hitset2 = set(keywords)
            def _score_final(it):
                t = f"{it['title']} {it.get('summary','')}"
                return compute_score(t, cfg) + apply_unrelated_penalty(hitset2, t, cfg)
            filtered.sort(key=_score_final, reverse=True)

        # 3) ì „ì—­ URL ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ìƒìœ„ Nê°œ ë³´ì¡´
        kept_unique = []
        for it in filtered:
            nu = normalize_url(it["link"])
            if nu in global_seen_urls:
                continue
            kept_unique.append(it)
            global_seen_urls.add(nu)
            if len(kept_unique) >= cfg["app"]["max_items_per_subcategory"]:
                break

        grouped[major][minor] = kept_unique
        total_kept += len(kept_unique)
        if selection_mode == "recent":
            print(f"[KEEP] {major}/{minor}: raw={before}, deduped={after_dedupe}, kept_unique={len(kept_unique)}")
        else:
            print(f"[KEEP] {major}/{minor}: raw={before}, deduped={after_dedupe}, ai_used_now={ai_used_now}, kept_unique={len(kept_unique)}")

    # ---------------- ìµœì¢… ë‹¨ê³„: ì œëª© ìœ ì‚¬ë„ ì „ì—­ ì¤‘ë³µ ì œê±° ----------------
    all_final = []
    for major, minors in grouped.items():
        for minor, items in minors.items():
            all_final.extend(items)

    deduped_final = dedupe_by_title_similarity(all_final, threshold=0.88)

    # ì¹´í…Œê³ ë¦¬ êµ¬ì¡°ë¡œ ì¬êµ¬ì„±
    grouped_clean = {}
    for it in deduped_final:
        major, minor = it.get("major"), it.get("minor")
        grouped_clean.setdefault(major, {}).setdefault(minor, []).append(it)

    # HTML ìƒì„±
    html = make_html_email(grouped_clean, cfg, start_dt, end_dt)

    # ë¯¸ë¦¬ë³´ê¸° ì €ì¥
    try:
        with open("email_preview.html", "w", encoding="utf-8") as f:
            f.write(html)
        print("[INFO] Saved email_preview.html")
    except Exception as ex:
        print(f"[WARN] preview save failed: {ex}")

    print(f"[SUMMARY] total_raw={total_raw}, total_kept={total_kept}, final={len(deduped_final)}, ai_used_total={ai_used}")
    if len(deduped_final) == 0:
        print("[INFO] No items kept; sending email anyway (empty) to validate SMTP...")

    # ë©”ì¼ ì „ì†¡
    try:
        send_email(html, cfg)
        print(f"[OK] Sent {len(deduped_final)} items.")
    except Exception as ex:
        print(f"[ERROR] send_email failed: {ex}")
        raise


def main():
    if "--once" in sys.argv or BlockingScheduler is None:
        run_once()
        return
    cfg = load_config()
    tz = pytz.timezone(cfg["app"]["timezone"])
    sched = BlockingScheduler(timezone=tz)
    sched.add_job(run_once, 'cron', hour=cfg["app"]["run_time_hour"], minute=0)
    print("[Scheduler] Started. Will run daily at %02d:00 %s." % (cfg["app"]["run_time_hour"], cfg["app"]["timezone"]))
    try:
        sched.start()
    except (KeyboardInterrupt, SystemExit):
        pass


if __name__ == "__main__":
    main()
