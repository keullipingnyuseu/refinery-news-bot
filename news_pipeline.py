# -*- coding: utf-8 -*-
"""
news_pipeline.py (ì œëª©ë§Œ í‘œì‹œ / ì „ì—­ URL+ì œëª© ìœ ì‚¬ë„ ì¤‘ë³µ ì œê±° / AI ìµœì†Œí™”)
- Google News(RSS) ìˆ˜ì§‘
- ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì‚¬ì „ ì°¨ë‹¨ â†’ (ì„ íƒ) AI ê´€ë ¨ì„± í•„í„°(ìƒìœ„ Nê°œë§Œ) â†’ ë“ì /ì •ë ¬/TopN
- HTML ë©”ì¼: ì œëª©, ê²Œì‹œ ì‹œê°(KST), ë§í¬ ë²„íŠ¼  â€» ìš”ì•½/ë³¸ë¬¸ ë¯¸í‘œì‹œ
"""

import os, sys, smtplib, pytz, yaml, feedparser, requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from urllib.parse import quote_plus

from rapidfuzz.distance import Levenshtein

from utils.scoring import compute_score, apply_unrelated_penalty
from utils.dedupe import dedupe_items, normalize_url   # URL ì „ì—­ ì¤‘ë³µ ì œê±°ì— ì‚¬ìš©
from utils.relevance import is_relevant                # AI/íœ´ë¦¬ìŠ¤í‹± ê´€ë ¨ì„± íŒì •

try:
    from apscheduler.schedulers.blocking import BlockingScheduler
except Exception:
    BlockingScheduler = None

# ----------------- Helpers -----------------

def load_config(path="config.yaml"):
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)

def google_news_rss(query, cfg):
    base = cfg["sources"]["google_news"]["base"]
    params = {
        "q": f'{query} when:1d',
        "hl": cfg["sources"]["google_news"]["hl"],
        "gl": cfg["sources"]["google_news"]["gl"],
        "ceid": cfg["sources"]["google_news"]["ceid"],
    }
    q = "&".join([f"{k}={quote_plus(v)}" for k,v in params.items()])
    url = f"{base}?{q}"
    headers = {"User-Agent": "Mozilla/5.0 (refinery-news-bot; +https://github.com)"}
    r = requests.get(url, headers=headers, timeout=15)
    r.raise_for_status()
    return feedparser.parse(r.text)

def extract_text(entry):
    title = entry.get("title", "")
    summary_html = entry.get("summary", "")
    try:
        summary = BeautifulSoup(summary_html, "html5lib").get_text(" ", strip=True)
    except Exception:
        summary = BeautifulSoup(summary_html, "html.parser").get_text(" ", strip=True)
    return title, summary

def is_block_domain(link, cfg):
    for d in cfg["filters"]["block_domains"]:
        if d in link:
            return True
    return False

def within_window(published, tz, start_dt, end_dt):
    if not published:
        return True
    try:
        dt = datetime(*published[:6], tzinfo=pytz.utc).astimezone(tz)
        return start_dt <= dt <= end_dt
    except Exception:
        return True

def to_local_str(published, tz):
    try:
        dt = datetime(*published[:6], tzinfo=pytz.utc).astimezone(tz)
        return dt.strftime("%Y-%m-%d %H:%M KST")
    except Exception:
        return "ì‹œê°„ ì •ë³´ ì—†ìŒ"

# ì œëª© ìœ ì‚¬ë„ ì „ì—­ ì¤‘ë³µ ì œê±° (Levenshtein similarity)
def dedupe_by_title_similarity(items, threshold=0.88):
    result = []
    seen_titles = []
    for it in items:
        title = (it.get("title") or "").strip()
        dup = False
        for seen in seen_titles:
            # ìœ ì‚¬ë„ = 1 - (edit distance / max len)
            sim = 1 - (Levenshtein.distance(title, seen) / max(len(title), len(seen) or 1))
            if sim >= threshold:
                dup = True
                break
        if not dup:
            result.append(it)
            seen_titles.append(title)
    return result

# ----------------- Pipeline pieces -----------------

def build_query_terms(taxonomy_item):
    return taxonomy_item["keywords"]

def block_by_keywords(title, summary, cfg):
    low = f"{title} {summary}".lower()
    for w in cfg["filters"].get("block_keywords", []):
        if w.lower() in low:
            return True
    return False

def score_and_filter(items, hit_keywords, cfg):
    scored = []
    for it in items:
        text = f'{it["title"]} {it.get("summary","")}'
        s = compute_score(text, cfg)
        s += apply_unrelated_penalty(hit_keywords, text, cfg)
        if s < 0:
            continue
        it["score"] = s
        scored.append(it)
    return sorted(scored, key=lambda x: x["score"], reverse=True)

def make_html_email(grouped, cfg, start_dt, end_dt):
    # ì œëª© + ê²Œì‹œ ì‹œê° + ë§í¬ ë²„íŠ¼ë§Œ ì¶œë ¥
    head = f"""
    <html><body style="font-family:Arial,Helvetica,sans-serif;">
      <h2>ì •ìœ  ë‰´ìŠ¤ ìš”ì•½ ({start_dt.strftime('%Y-%m-%d %H:%M')} ~ {end_dt.strftime('%Y-%m-%d %H:%M')} KST)</h2>
      <p style="color:#666;">ì •ìœ ì‚¬ ì§ì› ê´€ì  ìœ ì˜ë¯¸ ê¸°ì‚¬ë§Œ ì„ ë³„í–ˆìŠµë‹ˆë‹¤. (ê° ì¹´ë“œì— ê²Œì‹œ ì‹œê° í‘œì‹œ)</p>
    """
    cards = []
    for major, minors in grouped.items():
        cards.append(f'<h3 style="border-bottom:2px solid #eee;padding-bottom:4px;">{major}</h3>')
        for minor, items in minors.items():
            if not items:
                continue
            cards.append(f'<h4 style="margin:10px 0 6px 0;color:#0a4;">{minor}</h4>')
            for it in items:
                posted = it.get("published_local", "ì‹œê°„ ì •ë³´ ì—†ìŒ")
                cards.append(f"""
                <div style="border:1px solid #eee;border-radius:10px;padding:12px;margin:8px 0;">
                  <div style="color:#888;font-size:0.9em;margin-bottom:4px;">ğŸ“… {posted}</div>
                  <div style="font-weight:600;margin-bottom:10px;">{it['title']}</div>
                  <a style="display:inline-block;background:#1565C0;color:#fff;padding:8px 12px;border-radius:6px;text-decoration:none;"
                     href="{it['link']}" target="_blank" rel="noopener">ì›ë¬¸ ë³´ê¸°</a>
                </div>
                """)
    tail = "</body></html>"
    return head + "\n".join(cards) + tail

def send_email(html, cfg):
    print(f"[SMTP] host={cfg['email']['smtp_host']} port={cfg['email']['smtp_port']} tls={cfg['email']['use_tls']}")
    msg = MIMEMultipart("alternative")
    subject = f"{cfg['email']['subject_prefix']} {datetime.now().strftime('%Y-%m-%d')}"
    msg["Subject"] = subject
    msg["From"] = f"{cfg['email']['from_name']} <{cfg['email']['from_addr']}>"
    msg["To"] = ", ".join(cfg["email"]["to_addrs"])
    msg.attach(MIMEText(html, "html", "utf-8"))

    server = smtplib.SMTP(cfg["email"]["smtp_host"], cfg["email"]["smtp_port"], timeout=20)
    if cfg["email"]["use_tls"]:
        server.starttls()
    pwd = os.getenv(cfg["email"]["password_env"], "")
    if cfg["email"]["username"]:
        print(f"[SMTP] login as {cfg['email']['username']} (pwd={'SET' if bool(pwd) else 'EMPTY'})")
        server.login(cfg["email"]["username"], pwd)
    server.sendmail(cfg["email"]["from_addr"], cfg["email"]["to_addrs"], msg.as_string())
    server.quit()

# ----------------- Main run -----------------

def run_once():
    cfg = load_config()
    tz = pytz.timezone(cfg["app"]["timezone"])
    now = datetime.now(tz)
    end_dt = now
    start_dt = end_dt - timedelta(hours=cfg["app"]["lookback_hours"])

    print(f"[INFO] Window: {start_dt} ~ {end_dt} {cfg['app']['timezone']}")
    taxonomy = cfg["taxonomy"]
    grouped = {}
    total_raw = 0
    total_kept = 0

    # ì „ì—­ URL ì¤‘ë³µ ë°©ì§€ìš© ì„¸íŠ¸ (ì •ê·œí™” URL ê¸°ì¤€)
    global_seen_urls = set()

    # AI ì‚¬ìš© ì˜ˆì‚°
    ai_budget = int(cfg["openai"].get("relevance_max_checks", 30)) if cfg.get("openai") else 0
    ai_used = 0
    use_ai = bool(cfg.get("openai", {}).get("enable_ai_filter", False))

    for tax in taxonomy:
        major, minor = tax["major"], tax["minor"]
        grouped.setdefault(major, {})
        grouped[major].setdefault(minor, [])
        keywords = build_query_terms(tax)

        bucket = []
        for kw in keywords:
            try:
                d = google_news_rss(kw, cfg)
                got = len(d.entries)
                print(f"[FETCH] {major}/{minor}/{kw}: entries={got}")
                items = []
                for e in d.entries:
                    link = e.get("link", "")
                    if not link or is_block_domain(link, cfg):
                        continue
                    title, summary = extract_text(e)
                    if not title:
                        continue
                    if not within_window(getattr(e, "published_parsed", None), tz, start_dt, end_dt):
                        continue
                    if block_by_keywords(title, summary, cfg):
                        continue
                    published_local = to_local_str(getattr(e, "published_parsed", None), tz)
                    items.append({
                        "title": title,
                        "summary": summary,   # ë‚´ë¶€ ìŠ¤ì½”ì–´ë§ìš©, ë Œë”ë§ì—ëŠ” ë¯¸ì‚¬ìš©
                        "link": link,
                        "published": getattr(e, "published", ""),
                        "published_local": published_local,
                        "source": getattr(e, "source", {}).get("title") if hasattr(e, "source") else "",
                        "major": major,       # ìµœì¢… ì¬ê·¸ë£¹í•‘ì„ ìœ„í•´ ë³´ê´€
                        "minor": minor
                    })
                total_raw += len(items)
                bucket.extend(items)
            except Exception as ex:
                print(f"[WARN] fetch failed for {kw}: {ex}")

        # 1) ì†Œë¶„ë¥˜ ë‚´ ì¤‘ë³µ ì œê±°
        before = len(bucket)
        bucket = dedupe_items(bucket)
        after_dedupe = len(bucket)

        # 2) íœ´ë¦¬ìŠ¤í‹± í”„ë¦¬-ìŠ¤ì½”ì–´ ì •ë ¬
        prelims = []
        hitset = set(keywords)
        for it in bucket:
            txt = f"{it['title']} {it.get('summary','')}"
            pre = compute_score(txt, cfg) + apply_unrelated_penalty(hitset, txt, cfg)
            it["_pre_score"] = pre
            prelims.append(it)
        prelims.sort(key=lambda x: x["_pre_score"], reverse=True)

        # 3) ìƒìœ„ Nê°œë§Œ AI í•„í„°, ë‚˜ë¨¸ì§€ëŠ” íœ´ë¦¬ìŠ¤í‹±
        global_limit = int(cfg.get("openai", {}).get("relevance_max_checks", 30))
        can_use = max(0, global_limit - ai_used)
        top_n = min(can_use, len(prelims))

        filtered = []
        cfg_no_ai = {**cfg, "openai": {**cfg.get("openai", {}), "enable_ai_filter": False}}
        for idx, it in enumerate(prelims):
            txt = f"{it['title']}. {it.get('summary','')}"
            if idx < top_n and use_ai:
                rel = is_relevant(txt, cfg)
                ai_used += 1
            else:
                rel = is_relevant(txt, cfg_no_ai)  # íœ´ë¦¬ìŠ¤í‹±ë§Œ
            if rel:
                filtered.append(it)

        # 4) ìµœì¢… ìŠ¤ì½”ì–´ë§
        bucket = score_and_filter(filtered, hitset, cfg)

        # 5) ì „ì—­ URL ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ìƒìœ„ Nê°œ ë³´ì¡´
        kept_unique = []
        for it in bucket:
            nu = normalize_url(it["link"])
            if nu in global_seen_urls:
                continue  # ì´ë¯¸ ë‹¤ë¥¸ ëŒ€/ì¤‘ë¶„ë¥˜ì—ì„œ ì±„íƒë¨ â†’ ìŠ¤í‚µ
            kept_unique.append(it)
            global_seen_urls.add(nu)
            if len(kept_unique) >= cfg["app"]["max_items_per_subcategory"]:
                break

        grouped[major][minor] = kept_unique
        total_kept += len(kept_unique)
        print(f"[KEEP] {major}/{minor}: raw={before}, deduped={after_dedupe}, "
              f"ai_used_now={min(top_n,len(prelims)) if use_ai else 0}, kept_unique={len(kept_unique)}")

    # ---------------- ìµœì¢… ë‹¨ê³„: ì œëª© ìœ ì‚¬ë„ ì „ì—­ ì¤‘ë³µ ì œê±° ----------------
    # 6) ëª¨ë“  ìµœì¢… ì•„ì´í…œ í‰íƒ„í™”
    all_final = []
    for major, minors in grouped.items():
        for minor, items in minors.items():
            all_final.extend(items)

    # 7) ì œëª© ìœ ì‚¬ë„ ê¸°ë°˜ ì „ì—­ dedupe (threshold=0.88 ê¶Œì¥)
    deduped_final = dedupe_by_title_similarity(all_final, threshold=0.88)

    # 8) ì¹´í…Œê³ ë¦¬ êµ¬ì¡°ë¡œ ì¬êµ¬ì„± (ì› ì†Œì† ìœ ì§€)
    grouped_clean = {}
    for it in deduped_final:
        major, minor = it.get("major"), it.get("minor")
        grouped_clean.setdefault(major, {}).setdefault(minor, []).append(it)

    # HTML ìƒì„± (ì œëª©ë§Œ í‘œì‹œ)
    html = make_html_email(grouped_clean, cfg, start_dt, end_dt)

    # ë¯¸ë¦¬ë³´ê¸° ì €ì¥
    try:
        with open("email_preview.html", "w", encoding="utf-8") as f:
            f.write(html)
        print("[INFO] Saved email_preview.html")
    except Exception as ex:
        print(f"[WARN] preview save failed: {ex}")

    print(f"[SUMMARY] total_raw={total_raw}, total_kept={total_kept}, ai_used={ai_used}, final={len(deduped_final)}")
    if len(deduped_final) == 0:
        print("[INFO] No items kept; sending email anyway (empty) to validate SMTP...")

    # ë©”ì¼ ì „ì†¡
    try:
        send_email(html, cfg)
        print(f"[OK] Sent {len(deduped_final)} items.")
    except Exception as ex:
        print(f"[ERROR] send_email failed: {ex}")
        raise

def main():
    if "--once" in sys.argv or BlockingScheduler is None:
        run_once()
        return
    cfg = load_config()
    tz = pytz.timezone(cfg["app"]["timezone"])
    sched = BlockingScheduler(timezone=tz)
    sched.add_job(run_once, 'cron', hour=cfg["app"]["run_time_hour"], minute=0)
    print("[Scheduler] Started. Will run daily at %02d:00 %s." % (cfg["app"]["run_time_hour"], cfg["app"]["timezone"]))
    try:
        sched.start()
    except (KeyboardInterrupt, SystemExit):
        pass

if __name__ == "__main__":
    main()
