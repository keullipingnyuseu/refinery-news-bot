# -*- coding: utf-8 -*-
"""
news_pipeline.py (ìˆ˜ìš”Â·ê°€ê²© ì‹œê·¸ë„ ì¤‘ì‹¬ / AI ìµœì†Œ ì‚¬ìš© / ì „ì—­ ì œëª© ìœ ì‚¬ë„ dedupe)
- Google News(RSS) ìˆ˜ì§‘
- ë„ë©”ì¸/ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì°¨ë‹¨ â†’ (íœ´ë¦¬ìŠ¤í‹± ìŠ¤ì½”ì–´) â†’ ìƒìœ„ Nê°œë§Œ AI ê´€ë ¨ì„± â†’ ìµœì¢… ì •ë ¬/ì„ ë°œ
- ë©”ì¼ ì¹´ë“œ: ì œëª©, ê²Œì‹œ ì‹œê°(KST), ì›ë¬¸ ë§í¬
- ìµœì¢… ë‹¨ê³„: ì œëª© ìœ ì‚¬ë„ ì „ì—­ dedupe
"""

import os, sys, smtplib, pytz, yaml, feedparser, requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from urllib.parse import quote_plus

from utils.scoring import compute_score, apply_unrelated_penalty
from utils.relevance import is_relevant
from utils.dedupe import dedupe_items, normalize_url, dedupe_by_title_similarity

try:
    from apscheduler.schedulers.blocking import BlockingScheduler
except Exception:
    BlockingScheduler = None

# ----------------- helpers -----------------

def load_config(path="config.yaml"):
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)

def google_news_rss(query, cfg):
    base = cfg["sources"]["google_news"]["base"]
    params = {
        "q": f'{query} when:1d',
        "hl": cfg["sources"]["google_news"]["hl"],
        "gl": cfg["sources"]["google_news"]["gl"],
        "ceid": cfg["sources"]["google_news"]["ceid"],
    }
    q = "&".join([f"{k}={quote_plus(v)}" for k, v in params.items()])
    url = f"{base}?{q}"
    headers = {"User-Agent": "Mozilla/5.0 (refinery-news-bot; +github)"}
    r = requests.get(url, headers=headers, timeout=15)
    r.raise_for_status()
    return feedparser.parse(r.text)

def extract_text(entry):
    title = entry.get("title", "")
    summary_html = entry.get("summary", "")
    try:
        summary = BeautifulSoup(summary_html, "html5lib").get_text(" ", strip=True)
    except Exception:
        summary = BeautifulSoup(summary_html, "html.parser").get_text(" ", strip=True)
    return title, summary

def is_block_domain(link, cfg):
    for d in cfg["filters"]["block_domains"]:
        if d in link:
            return True
    return False

def within_window(published, tz, start_dt, end_dt):
    if not published:
        return True
    try:
        dt = datetime(*published[:6], tzinfo=pytz.utc).astimezone(tz)
        return start_dt <= dt <= end_dt
    except Exception:
        return True

def to_local_str(published, tz):
    try:
        dt = datetime(*published[:6], tzinfo=pytz.utc).astimezone(tz)
        return dt.strftime("%Y-%m-%d %H:%M KST")
    except Exception:
        return "ì‹œê°„ ì •ë³´ ì—†ìŒ"

def published_dt_kst(published_parsed, tz):
    try:
        if published_parsed:
            return datetime(*published_parsed[:6], tzinfo=pytz.utc).astimezone(tz)
    except Exception:
        pass
    return datetime(1970,1,1, tzinfo=tz)

def make_html_email(grouped, cfg, start_dt, end_dt):
    head = f"""
    <html><body style="font-family:Arial,Helvetica,sans-serif;">
      <h2>ì •ìœ  ë‰´ìŠ¤ ìš”ì•½ ({start_dt.strftime('%Y-%m-%d %H:%M')} ~ {end_dt.strftime('%Y-%m-%d %H:%M')} KST)</h2>
      <p style="color:#666;">ìˆ˜ìš”/ê°€ê²© ë³€ë™ì— ì§ê²°ëœ ê¸°ì‚¬ ìœ„ì£¼ë¡œ ì„ ë³„í–ˆìŠµë‹ˆë‹¤. (ê²Œì‹œ ì‹œê° í‘œê¸°)</p>
    """
    cards = []
    for major, minors in grouped.items():
        cards.append(f'<h3 style="border-bottom:2px solid #eee;padding-bottom:4px;">{major}</h3>')
        for minor, items in minors.items():
            if not items:
                continue
            cards.append(f'<h4 style="margin:10px 0 6px 0;color:#0a4;">{minor}</h4>')
            for it in items:
                posted = it.get("published_local", "ì‹œê°„ ì •ë³´ ì—†ìŒ")
                cards.append(f"""
                <div style="border:1px solid #eee;border-radius:10px;padding:12px;margin:8px 0;">
                  <div style="color:#888;font-size:0.9em;margin-bottom:4px;">ğŸ“… {posted}</div>
                  <div style="font-weight:600;margin-bottom:10px;">{it['title']}</div>
                  <a style="display:inline-block;background:#1565C0;color:#fff;padding:8px 12px;border-radius:6px;text-decoration:none;"
                     href="{it['link']}" target="_blank" rel="noopener">ì›ë¬¸ ë³´ê¸°</a>
                </div>
                """)
    tail = "</body></html>"
    return head + "\n".join(cards) + tail

def send_email(html, cfg):
    print(f"[SMTP] host={cfg['email']['smtp_host']} port={cfg['email']['smtp_port']} tls={cfg['email']['use_tls']}")
    msg = MIMEMultipart("alternative")
    subject = f"{cfg['email']['subject_prefix']} {datetime.now().strftime('%Y-%m-%d')}"
    msg["Subject"] = subject
    msg["From"] = f"{cfg['email']['from_name']} <{cfg['email']['from_addr']}>"
    msg["To"] = ", ".join(cfg["email"]["to_addrs"])
    msg.attach(MIMEText(html, "html", "utf-8"))

    server = smtplib.SMTP(cfg["email"]["smtp_host"], cfg["email"]["smtp_port"], timeout=20)
    if cfg["email"]["use_tls"]:
        server.starttls()
    pwd = os.getenv(cfg["email"]["password_env"], "")
    if cfg["email"]["username"]:
        print(f"[SMTP] login as {cfg['email']['username']} (pwd={'SET' if bool(pwd) else 'EMPTY'})")
        server.login(cfg["email"]["username"], pwd)
    server.sendmail(cfg["email"]["from_addr"], cfg["email"]["to_addrs"], msg.as_string())
    server.quit()

# ----------------- main -----------------

def run_once():
    cfg = load_config()
    tz = pytz.timezone(cfg["app"]["timezone"])
    now = datetime.now(tz)
    end_dt = now
    start_dt = end_dt - timedelta(hours=cfg["app"]["lookback_hours"])

    selection_mode = (cfg.get("app", {}).get("selection_mode") or "scored").lower()

    print(f"[INFO] Window: {start_dt} ~ {end_dt} {cfg['app']['timezone']} (mode={selection_mode})")
    taxonomy = cfg["taxonomy"]
    grouped = {}
    total_raw = 0
    total_kept = 0

    global_seen_urls = set()

    ai_budget = int(cfg.get("openai", {}).get("relevance_max_checks", 40))
    ai_used = 0
    use_ai = bool(cfg.get("openai", {}).get("enable_ai_filter", False))

    for tax in taxonomy:
        major, minor = tax["major"], tax["minor"]
        grouped.setdefault(major, {})
        grouped[major].setdefault(minor, [])
        keywords = tax["keywords"]

        bucket = []
        for kw in keywords:
            try:
                d = google_news_rss(kw, cfg)
                print(f"[FETCH] {major}/{minor}/{kw}: entries={len(d.entries)}")
                items = []
                for e in d.entries:
                    link = e.get("link", "")
                    if not link or is_block_domain(link, cfg):
                        continue
                    title, summary = extract_text(e)
                    if not title:
                        continue
                    pub_p = getattr(e, "published_parsed", None)
                    if not within_window(pub_p, tz, start_dt, end_dt):
                        continue
                    # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì¦‰ì‹œ ì»·
                    low = f"{title} {summary}".lower()
                    if any(w.lower() in low for w in cfg["filters"].get("block_keywords", [])):
                        continue
                    bucket.append({
                        "title": title,
                        "summary": summary,  # ë‚´ë¶€ ìŠ¤ì½”ì–´ ì°¸ê³ ìš©
                        "link": link,
                        "published": getattr(e, "published", ""),
                        "published_local": to_local_str(pub_p, tz),
                        "published_dt": published_dt_kst(pub_p, tz),
                        "major": major,
                        "minor": minor
                    })
                total_raw += len(items)
                # ìœ„ì—ì„œ items ë¡œ ëˆ„ì í•˜ì§€ ì•Šê³  ë°”ë¡œ bucketì— append í–ˆìœ¼ë‹ˆ total_rawëŠ” ì•„ë˜ë¡œ ë³´ì •
                total_raw += len(bucket)
            except Exception as ex:
                print(f"[WARN] fetch failed for {kw}: {ex}")

        # 1) ì†Œë¶„ë¥˜ ë‚´ dedupe
        before = len(bucket)
        bucket = dedupe_items(bucket)
        after_dedupe = len(bucket)

        if selection_mode == "scored":
            # 2) íœ´ë¦¬ìŠ¤í‹± í”„ë¦¬-ìŠ¤ì½”ì–´ ì •ë ¬
            hitset = set(keywords)
            for it in bucket:
                txt = f"{it['title']} {it.get('summary','')}"
                it["_pre_score"] = compute_score(txt, cfg) + apply_unrelated_penalty(hitset, txt, cfg)
            bucket.sort(key=lambda x: x["_pre_score"], reverse=True)

            # 3) ìƒìœ„ Nê°œë§Œ AI í•„í„°
            can_use = max(0, ai_budget - ai_used)
            top_n = min(can_use, len(bucket))
            filtered = []
            cfg_no_ai = {**cfg, "openai": {**cfg.get("openai", {}), "enable_ai_filter": False}}
            ai_used_now = 0
            for idx, it in enumerate(bucket):
                txt = f"{it['title']}. {it.get('summary','')}"
                if idx < top_n and use_ai:
                    rel = is_relevant(txt, cfg)
                    ai_used += 1
                    ai_used_now += 1
                else:
                    rel = is_relevant(txt, cfg_no_ai)  # íœ´ë¦¬ìŠ¤í‹± í´ë°±
                if rel:
                    filtered.append(it)

            # 4) ìµœì¢… ì •ë ¬: ì‹œê·¸ë„ ì ìˆ˜ ìš°ì„ , ë™ì ì´ë©´ ìµœì‹ ìˆœ
            def _final_key(it):
                s = it.get("_pre_score", 0.0)
                t = it.get("published_dt")
                return (s, t)
            filtered.sort(key=_final_key, reverse=True)
        else:
            # selection_mode == "recent": ìµœì‹ ìˆœë§Œ
            filtered = sorted(bucket, key=lambda it: it.get("published_dt"), reverse=True)
            ai_used_now = 0

        # 5) ì „ì—­ URL dedupe + ìƒí•œ ë³´ì¡´
        kept_unique = []
        for it in filtered:
            nu = normalize_url(it["link"])
            if nu in global_seen_urls:
                continue
            kept_unique.append(it)
            global_seen_urls.add(nu)
            if len(kept_unique) >= cfg["app"]["max_items_per_subcategory"]:
                break

        grouped[major][minor] = kept_unique
        total_kept += len(kept_unique)
        print(f"[KEEP] {major}/{minor}: raw={before}, deduped={after_dedupe}, ai_used_now={ai_used_now}, kept={len(kept_unique)}")

    # ---------------- ìµœì¢…: ì œëª© ìœ ì‚¬ë„ ì „ì—­ dedupe ----------------
    all_final = []
    for major, minors in grouped.items():
        for minor, items in minors.items():
            all_final.extend(items)

    final_dedup = dedupe_by_title_similarity(all_final, threshold=0.88)

    # ì¹´í…Œê³ ë¦¬ ì¬êµ¬ì„±
    grouped_clean = {}
    for it in final_dedup:
        grouped_clean.setdefault(it["major"], {}).setdefault(it["minor"], []).append(it)

    # HTML ìƒì„±/ì €ì¥/ì „ì†¡
    html = make_html_email(grouped_clean, cfg, start_dt, end_dt)
    try:
        with open("email_preview.html", "w", encoding="utf-8") as f:
            f.write(html)
        print("[INFO] Saved email_preview.html")
    except Exception as ex:
        print(f"[WARN] preview save failed: {ex}")

    print(f"[SUMMARY] total_raw={total_raw}, total_kept={total_kept}, final={len(final_dedup)}, ai_used_total={ai_used}")

    try:
        send_email(html, cfg)
        print(f"[OK] Sent {len(final_dedup)} items.")
    except Exception as ex:
        print(f"[ERROR] send_email failed: {ex}")
        raise

def main():
    if "--once" in sys.argv or BlockingScheduler is None:
        run_once()
        return
    cfg = load_config()
    tz = pytz.timezone(cfg["app"]["timezone"])
    sched = BlockingScheduler(timezone=tz)
    sched.add_job(run_once, 'cron', hour=cfg["app"]["run_time_hour"], minute=0)
    print(f"[Scheduler] Started. Will run daily at %02d:00 %s." % (cfg["app"]["run_time_hour"], cfg["app"]["timezone"]))
    try:
        sched.start()
    except (KeyboardInterrupt, SystemExit):
        pass

if __name__ == "__main__":
    main()
